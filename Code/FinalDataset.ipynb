{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57025715-9914-47f3-8540-b7b8c8321010",
   "metadata": {},
   "source": [
    "# Cluster-Track Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f9bd5-8e82-4c42-84e6-8e68207b5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Joint clustering-tracking for LiDAR frames.\n",
    "- Per-point encoder produces embeddings for spatial-semantic assignment.\n",
    "- Two-stage assignment: CORE (tight gate + margin) then GROW (looser, density- and Mahalanobis-aware).\n",
    "- Optional births/deaths keep tracks consistent through occlusions and merges.\n",
    "\n",
    "This snippet is the clean version: paths, plotting, exports and long training boilerplate are removed.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import math, time, json, random, re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class GeoMask:\n",
    "    range_max_m: float = 95.0\n",
    "    hfov_deg: float = 70.4\n",
    "    r_full_m: float = 30.0\n",
    "    center_at_far_deg: float = 5.0\n",
    "    taper_power: float = 5.0\n",
    "    use_planar_range: bool = True\n",
    "    use_simple_ground: bool = True\n",
    "    ground_z_margin: float = 0.12\n",
    "\n",
    "@dataclass\n",
    "class EvalCfg:\n",
    "    assign_max_dist: float = 3.0\n",
    "    adaptive_gate_k: float = 0.04\n",
    "    mahal_xy_thr: float = 2.8\n",
    "    mahal_eps: float = 1e-4\n",
    "    use_two_stage: bool = True\n",
    "    allow_births: bool = True\n",
    "    allow_deaths: bool = True\n",
    "    death_max_misses: int = 6\n",
    "    birth_min_pts: int = 28\n",
    "    birth_eps_xy: float = 0.9\n",
    "    birth_min_d_from_seeds: float = 1.2\n",
    "    birth_keep_if_dense_q: float = 0.50\n",
    "    r_split_near: float = 25.0\n",
    "    alpha_near: float = 0.60\n",
    "    alpha_far: float = 0.40\n",
    "    bg_gate_fraction: float = 0.75\n",
    "    low_density_factor: float = 0.40\n",
    "    far_close_ratio: float = 0.60\n",
    "    motion_smooth: float = 0.30\n",
    "    pca_low: float = 5.0\n",
    "    pca_high: float = 95.0\n",
    "    pca_min_span: float = 0.25\n",
    "    emb_batch: int = 32768\n",
    "    use_amp: bool = True\n",
    "\n",
    "@dataclass\n",
    "class BuildCfg:\n",
    "    stack_w: int = 4\n",
    "    margin: float = 0.05\n",
    "    min_points: int = 10\n",
    "    include_classes: Iterable[str] = (\"dog\", \"human\", \"atlas\")\n",
    "\n",
    "GMASK = GeoMask()\n",
    "ECFG  = EvalCfg()\n",
    "BCFG  = BuildCfg()\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TIME_DENOM = 100.0  # sin/cos time code scale\n",
    "\n",
    "# ----------------------------- I/O -----------------------------\n",
    "\n",
    "POINT_EXTS = [\".bin\", \".npz\", \".npy\", \".csv\", \".txt\", \".pcd\"]\n",
    "\n",
    "def _frame_idx_from_stem(stem: str) -> int:\n",
    "    m = re.search(r\"(\\d+)$\", stem)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def _find_points_file(points_dir: Path, stem: str) -> Optional[Path]:\n",
    "    for ext in POINT_EXTS:\n",
    "        p = points_dir / f\"{stem}{ext}\"\n",
    "        if p.exists(): return p\n",
    "    return None\n",
    "\n",
    "def read_points_auto(path: Path) -> np.ndarray:\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".bin\":\n",
    "        a = np.fromfile(path, dtype=np.float32)\n",
    "        a = a.reshape(-1, 4)[:, :3] if a.size % 4 == 0 else a[: (a.size//3)*3].reshape(-1, 3)\n",
    "        return a.astype(np.float32)\n",
    "    if suf == \".npz\":\n",
    "        d = np.load(path)\n",
    "        for k in (\"xyz\", \"points\", \"xyzi\", \"arr_0\"):\n",
    "            if k in d and d[k].ndim == 2 and d[k].shape[1] >= 3:\n",
    "                return d[k][:, :3].astype(np.float32)\n",
    "        return np.zeros((0, 3), np.float32)\n",
    "    if suf == \".npy\":\n",
    "        a = np.asarray(np.load(path))\n",
    "        if a.ndim == 2 and a.shape[1] >= 3: return a[:, :3].astype(np.float32)\n",
    "        if a.ndim == 2 and a.shape[0] >= 3: return a.T[:, :3].astype(np.float32)\n",
    "        return np.zeros((0, 3), np.float32)\n",
    "    if suf in {\".csv\", \".txt\"}:\n",
    "        try: a = np.loadtxt(path, delimiter=\",\")\n",
    "        except Exception: a = np.loadtxt(path)\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] >= 3: return a[:, :3].astype(np.float32)\n",
    "        if a.ndim == 2 and a.shape[0] >= 3: return a.T[:, :3].astype(np.float32)\n",
    "        return np.zeros((0, 3), np.float32)\n",
    "    if suf == \".pcd\":\n",
    "        try:\n",
    "            import open3d as o3d\n",
    "            o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n",
    "            pcd = o3d.io.read_point_cloud(str(path))\n",
    "            return np.asarray(pcd.points, dtype=np.float32)\n",
    "        except Exception:\n",
    "            return np.zeros((0, 3), np.float32)\n",
    "    return np.zeros((0, 3), np.float32)\n",
    "\n",
    "def load_label(path: Path) -> Dict:\n",
    "    try: return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception: return {}\n",
    "\n",
    "def parse_objects(meta: Dict) -> List[Dict]:\n",
    "    inc = {c.lower() for c in BCFG.include_classes}\n",
    "    out = []\n",
    "    for o in meta.get(\"objects\", []):\n",
    "        cls = str(o.get(\"class\", \"\")).lower().strip()\n",
    "        if cls == \"dog_atlas\": cls = \"atlas\"\n",
    "        if cls not in inc: continue\n",
    "        pos = np.array(o.get(\"position\", [0, 0, 0]), np.float32)\n",
    "        rot = np.array(o.get(\"rotation\", [0, 0, 0]), np.float32)\n",
    "        scale = np.array(o.get(\"scale\",    [0, 0, 0]), np.float32)\n",
    "        tid = int(o.get(\"track_id\", -1))\n",
    "        out.append({\"class\": cls, \"pos\": pos, \"yaw\": float(rot[-1]), \"scale\": scale, \"track_id\": tid})\n",
    "    return out\n",
    "\n",
    "# ----------------------------- Masks & PCA -----------------------------\n",
    "\n",
    "def _width_allowed_monotone(r: np.ndarray, r_full, r_max, hfov_half, center_far, power):\n",
    "    t = np.clip((r - r_full) / max(1e-6, (r_max - r_full)), 0.0, 1.0)\n",
    "    ease = np.power(1.0 - t, power)\n",
    "    theta = center_far + (hfov_half - center_far) * ease\n",
    "    theta[r <= r_full] = hfov_half\n",
    "    return r * np.tan(theta)\n",
    "\n",
    "def fov_mask(xyz: np.ndarray, cfg: GeoMask = GMASK) -> np.ndarray:\n",
    "    if xyz.size == 0: return np.zeros((0,), bool)\n",
    "    x, y, _ = xyz[:, 0], xyz[:, 1], xyz[:, 2]\n",
    "    r = np.sqrt(x*x + y*y) if cfg.use_planar_range else np.linalg.norm(xyz, axis=1)\n",
    "    m_range = (r <= float(cfg.range_max_m)) & np.isfinite(r)\n",
    "    hf = math.radians(cfg.hfov_deg * 0.5)\n",
    "    cf = math.radians(cfg.center_at_far_deg)\n",
    "    w = _width_allowed_monotone(r, cfg.r_full_m, cfg.range_max_m, hf, cf, cfg.taper_power)\n",
    "    m_width = np.abs(y) <= w\n",
    "    return m_range & m_width\n",
    "\n",
    "def simple_ground_mask(xyz: np.ndarray, z_margin: float = GMASK.ground_z_margin) -> np.ndarray:\n",
    "    if xyz.size == 0: return np.zeros((0,), bool)\n",
    "    x, y, z = xyz[:, 0], xyz[:, 1], xyz[:, 2]\n",
    "    r = np.sqrt(x*x + y*y)\n",
    "    bins = np.linspace(0, 95.0, 20)\n",
    "    keep = np.zeros(len(z), bool)\n",
    "    for i in range(len(bins)-1):\n",
    "        m = (r >= bins[i]) & (r < bins[i+1])\n",
    "        if not m.any(): continue\n",
    "        zg = np.percentile(z[m], 5)\n",
    "        keep[m] = z[m] > (zg + z_margin)\n",
    "    return keep\n",
    "\n",
    "def pca_tighten(pts: np.ndarray, low=5.0, high=95.0, min_span=0.25) -> np.ndarray:\n",
    "    if pts.shape[0] < 5: return pts\n",
    "    xy = pts[:, :2] - np.median(pts[:, :2], axis=0, keepdims=True)\n",
    "    C = np.cov(xy.T) + 1e-6 * np.eye(2, dtype=np.float32)\n",
    "    _, V = np.linalg.eigh(C)\n",
    "    xy_r = xy @ V\n",
    "    lo, hi = np.percentile(xy_r, low, axis=0), np.percentile(xy_r, high, axis=0)\n",
    "    span = np.maximum(hi - lo, min_span)\n",
    "    ok_xy = ((xy_r[:, 0] >= lo[0]) & (xy_r[:, 0] <= lo[0] + span[0]) &\n",
    "             (xy_r[:, 1] >= lo[1]) & (xy_r[:, 1] <= lo[1] + span[1]))\n",
    "    z = pts[:, 2]\n",
    "    zlo, zhi = np.percentile(z, low), np.percentile(z, high)\n",
    "    ok_z = np.ones_like(z, bool) if (zhi - zlo) < 1e-6 else ((z >= zlo) & (z <= zhi))\n",
    "    keep = ok_xy & ok_z\n",
    "    return pts if not np.any(keep) else pts[keep]\n",
    "\n",
    "def pca_tighten_eval(pts: np.ndarray) -> np.ndarray:\n",
    "    return pca_tighten(pts, ECFG.pca_low, ECFG.pca_high, ECFG.pca_min_span)\n",
    "\n",
    "# ----------------------------- Labels to points -----------------------------\n",
    "\n",
    "def points_in_cuboid(xyz: np.ndarray, center: np.ndarray, scale: np.ndarray, yaw: float, margin: float) -> np.ndarray:\n",
    "    if xyz.size == 0: return np.zeros((0,), bool)\n",
    "    c = np.asarray(center, np.float32).reshape(3)\n",
    "    s = np.asarray(scale,  np.float32).reshape(3)\n",
    "    hx, hy, hz = 0.5*np.abs(s) * (1.0 + margin)\n",
    "    if hz <= 0: hz = 5.0\n",
    "    if (hx < 1e-3) or (hy < 1e-3): return np.zeros((xyz.shape[0],), bool)\n",
    "    if abs(yaw) > math.pi: yaw = math.radians(yaw % 360.0)\n",
    "    cth, sth = math.cos(yaw), math.sin(yaw)\n",
    "    P = xyz - c[None, :]\n",
    "    X =  cth*P[:,0] + sth*P[:,1]\n",
    "    Y = -sth*P[:,0] + cth*P[:,1]\n",
    "    Z =  P[:,2]\n",
    "    return (np.abs(X)<=hx) & (np.abs(Y)<=hy) & (np.abs(Z)<=hz)\n",
    "\n",
    "def gt_assign_for_eval(xyz: np.ndarray, objs: List[Dict]) -> np.ndarray:\n",
    "    if xyz.shape[0] == 0 or not objs: return -np.ones((0,), np.int64)\n",
    "    centers = np.stack([o[\"pos\"][:2] for o in objs], 0).astype(np.float32)\n",
    "    tids = np.array([int(o[\"track_id\"]) for o in objs], np.int64)\n",
    "    radii = []\n",
    "    for o in objs:\n",
    "        sc = np.asarray(o.get(\"scale\", [1,1,1]), np.float32)\n",
    "        r = float(1.3 * float(np.max(np.abs(sc[:2])))) if sc.size >= 2 else 1.0\n",
    "        radii.append(r)\n",
    "    radii = np.array(radii, np.float32)\n",
    "    XY = xyz[:, :2]\n",
    "    d2 = ((XY[:, None, :] - centers[None, :, :])**2).sum(2)\n",
    "    j = np.argmin(d2, axis=1)\n",
    "    d = np.sqrt(d2[np.arange(xyz.shape[0]), j])\n",
    "    out = -np.ones((xyz.shape[0],), np.int64)\n",
    "    ok = d <= radii[j]\n",
    "    out[ok] = tids[j[ok]]\n",
    "    return out\n",
    "\n",
    "# ----------------------------- Encoder (eval-time) -----------------------------\n",
    "\n",
    "class SetEncoder(nn.Module):\n",
    "    \"\"\"Per-point MLP -> max-pool -> embedding head. Used both for per-point and seed prototypes.\"\"\"\n",
    "    def __init__(self, in_dim=7, hidden=256, emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.point_mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(True),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(True),\n",
    "            nn.Linear(hidden, 256),     nn.ReLU(True),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Linear(256, emb_dim), nn.LayerNorm(emb_dim))\n",
    "\n",
    "    def point_feats(self, X):  # (B,N,7) -> (B,N,256)\n",
    "        return self.point_mlp(X)\n",
    "\n",
    "    def pool_set(self, H):     # (B,N,256) -> (B,256)\n",
    "        return torch.max(H, dim=1).values\n",
    "\n",
    "    def forward(self, X):      # (B,N,7) -> (B,emb)\n",
    "        H = self.point_feats(X); Z = self.head(self.pool_set(H))\n",
    "        return F.normalize(Z, dim=1)\n",
    "\n",
    "def _time_code_scalar(t_idx: int, denom: float = TIME_DENOM) -> Tuple[np.float32, np.float32]:\n",
    "    tau = float(t_idx) / float(denom)\n",
    "    return np.float32(np.cos(tau)), np.float32(np.sin(tau))\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_points_pointwise(model: SetEncoder, P: np.ndarray, t_idx: int = 0, batch: int = None) -> np.ndarray:\n",
    "    if P.size == 0: return np.zeros((0, 128), np.float32)\n",
    "    batch = batch or ECFG.emb_batch\n",
    "    out = []\n",
    "    use_amp = ECFG.use_amp and torch.cuda.is_available()\n",
    "    for s in range(0, P.shape[0], batch):\n",
    "        e = min(P.shape[0], s+batch)\n",
    "        A = P[s:e].astype(np.float32)\n",
    "        x, y, z = A[:,0], A[:,1], A[:,2]\n",
    "        r = np.sqrt(x*x + y*y); th = np.arctan2(y, x)\n",
    "        tcos, tsin = _time_code_scalar(int(t_idx))\n",
    "        X = torch.from_numpy(np.stack([x,y,z,r,th,\n",
    "                                       np.full_like(r, tcos, np.float32),\n",
    "                                       np.full_like(r, tsin, np.float32)], 1)\n",
    "                             ).unsqueeze(0).to(DEVICE, non_blocking=True)\n",
    "        if use_amp:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                H = model.point_feats(X); Zp = model.head(H.squeeze(0))\n",
    "        else:\n",
    "            H = model.point_feats(X); Zp = model.head(H.squeeze(0))\n",
    "        out.append(F.normalize(Zp, dim=1).float().cpu().numpy())\n",
    "    return np.concatenate(out, 0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def seed_proto(model: SetEncoder, S: np.ndarray, t_idx: int) -> np.ndarray:\n",
    "    if S.shape[0] == 0: return np.zeros((128,), np.float32)\n",
    "    K = min(512, max(32, S.shape[0]))\n",
    "    A = S[np.random.choice(S.shape[0], size=K, replace=True)]\n",
    "    x,y,z = A[:,0], A[:,1], A[:,2]\n",
    "    r = np.sqrt(x*x + y*y); th = np.arctan2(y, x)\n",
    "    tcos, tsin = _time_code_scalar(int(t_idx))\n",
    "    X = torch.from_numpy(np.stack([x,y,z,r,th,\n",
    "                                   np.full_like(r, tcos, np.float32),\n",
    "                                   np.full_like(r, tsin, np.float32)], 1)\n",
    "                         ).unsqueeze(0).to(DEVICE, non_blocking=True)\n",
    "    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(ECFG.use_amp and torch.cuda.is_available())):\n",
    "        H = model.point_feats(X); Z = model.head(model.pool_set(H))\n",
    "    return F.normalize(Z.squeeze(0), dim=0).float().cpu().numpy()\n",
    "\n",
    "# ----------------------------- Density + helpers -----------------------------\n",
    "\n",
    "def knn_density_xy(Pxy: np.ndarray, k: int = 16) -> np.ndarray:\n",
    "    if Pxy.size == 0: return np.zeros((0,), np.float32)\n",
    "    try:\n",
    "        from scipy.spatial import cKDTree\n",
    "        tree = cKDTree(Pxy.astype(np.float64), leafsize=64)\n",
    "        k_eff = int(min(k + 1, max(2, len(Pxy))))\n",
    "        dists, _ = tree.query(Pxy, k=k_eff, workers=-1)\n",
    "        if dists.ndim == 1: dists = dists[:, None]\n",
    "        rk = np.maximum(dists[:, 1:].max(axis=1).astype(np.float32), 1e-3)\n",
    "        return (k / (np.pi * rk * rk)).astype(np.float32)\n",
    "    except Exception:\n",
    "        # Histogram fallback\n",
    "        nb = 128\n",
    "        H, xedges, yedges = np.histogram2d(Pxy[:,0], Pxy[:,1], bins=nb)\n",
    "        xi = np.clip(np.searchsorted(xedges, Pxy[:,0], side='right') - 1, 0, nb-1)\n",
    "        yi = np.clip(np.searchsorted(yedges, Pxy[:,1], side='right') - 1, 0, nb-1)\n",
    "        counts = H[xi, yi].astype(np.float32)\n",
    "        area = (xedges[1]-xedges[0]) * (yedges[1]-yedges[0]) or 1.0\n",
    "        return counts / np.float32(area)\n",
    "\n",
    "def _alpha_for_range(seeds_xy: np.ndarray) -> float:\n",
    "    if seeds_xy.size == 0: return ECFG.alpha_far\n",
    "    r = np.linalg.norm(seeds_xy, axis=1)\n",
    "    near_frac = (r < ECFG.r_split_near).mean() if r.size else 0.0\n",
    "    return float(near_frac*ECFG.alpha_near + (1.0-near_frac)*ECFG.alpha_far)\n",
    "\n",
    "# ----------------------------- Assignment (two-stage) -----------------------------\n",
    "\n",
    "def assign_two_stage(model: SetEncoder, P: np.ndarray, t_idx: int,\n",
    "                     seeds_xy: np.ndarray, seeds_emb: np.ndarray,\n",
    "                     mahal_state: Optional[Dict] = None) -> np.ndarray:\n",
    "    if P.shape[0] == 0 or seeds_xy.shape[0] == 0:\n",
    "        return np.full((P.shape[0],), -1, int)\n",
    "\n",
    "    Zp = embed_points_pointwise(model, P, t_idx)\n",
    "    Zp_n = Zp / (np.linalg.norm(Zp, axis=1, keepdims=True) + 1e-8)\n",
    "    Ze_n = seeds_emb / (np.linalg.norm(seeds_emb, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "    XY = P[:, :2].astype(np.float32)\n",
    "    d_xy = np.linalg.norm(XY[:, None, :] - seeds_xy[None, :, :], axis=2)\n",
    "    sim  = np.clip(Zp_n @ Ze_n.T, -1.0, 1.0)\n",
    "    d_emb = 1.0 - sim\n",
    "\n",
    "    alpha = _alpha_for_range(seeds_xy)\n",
    "    seed_r_med = np.median(np.linalg.norm(seeds_xy, axis=1)) if seeds_xy.size else 0.0\n",
    "    gate_xy = ECFG.assign_max_dist * (1.0 + ECFG.adaptive_gate_k * max(0.0, seed_r_med - 20.0))\n",
    "    d_blend = alpha*d_xy + (1.0 - alpha)*d_emb\n",
    "\n",
    "    # --- CORE (tight + margin) ---\n",
    "    j1   = np.argmin(d_blend, axis=1)\n",
    "    best = np.min(d_blend, axis=1)\n",
    "    if d_blend.shape[1] >= 2:\n",
    "        part2 = np.partition(d_blend, kth=1, axis=1)[:, :2]\n",
    "        second = part2.max(axis=1)\n",
    "        margin = second - best\n",
    "    else:\n",
    "        margin = np.full_like(best, 1.0, dtype=np.float32)\n",
    "\n",
    "    core_gate_xy = 0.75 * gate_xy\n",
    "    thr_far_core = (1.4 + 0.02 * max(0.0, seed_r_med - 25.0))\n",
    "    ok_xy  = d_xy[np.arange(d_xy.shape[0]), j1] <= core_gate_xy\n",
    "    ok_mix = best <= (alpha*core_gate_xy + (1.0 - alpha)*thr_far_core)\n",
    "    ok_mg  = margin >= 0.15\n",
    "\n",
    "    asg = np.full((P.shape[0],), -1, int)\n",
    "    core_mask = ok_xy & ok_mix & ok_mg\n",
    "    asg[core_mask] = j1[core_mask]\n",
    "\n",
    "    # Update seed stats from cores (EMA position + refreshed prototypes + Mahalanobis)\n",
    "    K = seeds_xy.shape[0]\n",
    "    mu_list, inv_list = [], []\n",
    "    for k in range(K):\n",
    "        hit = (asg == k)\n",
    "        if not np.any(hit):\n",
    "            mu_list.append(seeds_xy[k]); inv_list.append(np.eye(2, dtype=np.float32)); continue\n",
    "        cl = pca_tighten_eval(P[hit])\n",
    "        if cl.shape[0] == 0:\n",
    "            mu_list.append(seeds_xy[k]); inv_list.append(np.eye(2, dtype=np.float32)); continue\n",
    "        new_xy = np.median(cl[:, :2], 0).astype(np.float32)\n",
    "        seeds_xy[k] = (1.0-ECFG.motion_smooth)*seeds_xy[k] + ECFG.motion_smooth*new_xy\n",
    "        seeds_emb[k] = seed_proto(model, cl, t_idx)\n",
    "        xy = cl[:, :2].astype(np.float32)\n",
    "        mu = np.median(xy, axis=0).astype(np.float32)\n",
    "        ctr = xy - mu\n",
    "        if ctr.shape[0] < 8:\n",
    "            var = np.maximum(ctr.var(axis=0), ECFG.mahal_eps)\n",
    "            C = np.diag(var + ECFG.mahal_eps).astype(np.float32)\n",
    "        else:\n",
    "            C = np.cov(ctr.T, bias=False) + ECFG.mahal_eps*np.eye(2, dtype=np.float32)\n",
    "        try: Cinv = np.linalg.inv(C)\n",
    "        except np.linalg.LinAlgError: Cinv = np.linalg.pinv(C)\n",
    "        mu_list.append(mu); inv_list.append(Cinv)\n",
    "\n",
    "    if mahal_state is not None:\n",
    "        mahal_state[\"mu\"] = np.stack(mu_list, 0).astype(np.float32)\n",
    "        mahal_state[\"covinv\"] = np.stack(inv_list, 0).astype(np.float32)\n",
    "\n",
    "    # --- GROW (looser, density-aware, Mahalanobis-gated) ---\n",
    "    remain = (asg < 0)\n",
    "    if not np.any(remain): return asg\n",
    "\n",
    "    rho = knn_density_xy(XY); med_rho = float(np.median(rho)) if rho.size else 0.0\n",
    "    mu = np.stack(mu_list, 0).astype(np.float32); Cinv = np.stack(inv_list, 0).astype(np.float32)\n",
    "\n",
    "    grow_gate_xy = 1.15 * gate_xy\n",
    "    thr_far_grow = (1.7 + 0.02 * max(0.0, seed_r_med - 25.0))\n",
    "    low_den_thr = ECFG.low_density_factor * (med_rho + 1e-6)\n",
    "\n",
    "    idxs = np.where(remain)[0]\n",
    "    for ii in idxs:\n",
    "        d = XY[ii][None, :] - mu\n",
    "        q = np.einsum('ki,kij,kj->k', d, Cinv, d)\n",
    "        mxy = np.sqrt(np.maximum(0.0, q))\n",
    "        s_xy = d_xy[ii]\n",
    "        s_blend = alpha*s_xy + (1.0 - alpha)*(1.0 - sim[ii])\n",
    "\n",
    "        feas = (s_xy <= grow_gate_xy) & (mxy <= ECFG.mahal_xy_thr)\n",
    "        if not np.any(feas): continue\n",
    "        kbest = int(np.argmin(np.where(feas, s_blend, np.inf)))\n",
    "        if (rho[ii] < low_den_thr) and (s_xy[kbest] > 0.9*grow_gate_xy): continue\n",
    "        if s_blend[kbest] > (alpha*grow_gate_xy + (1.0 - alpha)*thr_far_grow): continue\n",
    "        asg[ii] = kbest\n",
    "\n",
    "    return asg\n",
    "\n",
    "# ----------------------------- Metrics -----------------------------\n",
    "\n",
    "def point_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    if y_true.size == 0 or y_pred.size == 0: return 0.0\n",
    "    return float(np.mean(y_true == y_pred))\n",
    "\n",
    "def bcubed_scores(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, float, float]:\n",
    "    T, P = y_true.copy(), y_pred.copy()\n",
    "    m = (T != -1) | (P != -1)\n",
    "    if not np.any(m): return 0.0, 0.0, 0.0\n",
    "    T, P = T[m], P[m]\n",
    "    N = len(T)\n",
    "    precs, recs = [], []\n",
    "    for i in range(N):\n",
    "        mask_p = (P == P[i]); mask_t = (T == T[i])\n",
    "        inter = np.sum(mask_p & mask_t)\n",
    "        precs.append(inter / max(1, np.sum(mask_p)))\n",
    "        recs.append(inter / max(1, np.sum(mask_t)))\n",
    "    Pm, Rm = float(np.mean(precs)), float(np.mean(recs))\n",
    "    F = 0.0 if (Pm + Rm) == 0 else (2*Pm*Rm)/(Pm + Rm)\n",
    "    return Pm, Rm, F\n",
    "\n",
    "# ----------------------------- Minimal eval loop (per recording) -----------------------------\n",
    "\n",
    "def eval_recording(model: SetEncoder, frames: List[Dict]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluates a single contiguous recording (list of {points_path, labels_path, frame_idx}).\"\"\"\n",
    "    if not frames: return {\"PointAcc\": 0.0, \"B3F1\": 0.0}\n",
    "\n",
    "    # Bootstrap seeds from first frame GT\n",
    "    first = frames[0]\n",
    "    P0 = read_points_auto(Path(first[\"points_path\"]))\n",
    "    if P0.size == 0: return {\"PointAcc\": 0.0, \"B3F1\": 0.0}\n",
    "    P0 = P0[fov_mask(P0, GMASK)]\n",
    "    if GMASK.use_simple_ground: P0 = P0[simple_ground_mask(P0, GMASK.ground_z_margin)]\n",
    "    objs0 = parse_objects(load_label(Path(first[\"labels_path\"])))\n",
    "    if not objs0: return {\"PointAcc\": 0.0, \"B3F1\": 0.0}\n",
    "\n",
    "    seeds_xy = np.stack([o[\"pos\"][:2] for o in objs0], 0).astype(np.float32)\n",
    "    seeds_emb = []\n",
    "    for o in objs0:\n",
    "        m0 = points_in_cuboid(P0, o[\"pos\"], o[\"scale\"], o[\"yaw\"], margin=BCFG.margin)\n",
    "        cl = pca_tighten(P0[m0]) if np.any(m0) else P0\n",
    "        if cl.shape[0] == 0: cl = P0\n",
    "        seeds_emb.append(seed_proto(model, cl, first[\"frame_idx\"]))\n",
    "    seeds_emb = np.stack(seeds_emb, 0)\n",
    "    miss_counts = [0] * seeds_xy.shape[0]\n",
    "    mahal_state = {\"mu\": None, \"covinv\": None}\n",
    "\n",
    "    accs, f1s = [], []\n",
    "    for m in frames:\n",
    "        P = read_points_auto(Path(m[\"points_path\"]))\n",
    "        if P.size == 0: continue\n",
    "        P = P[fov_mask(P, GMASK)]\n",
    "        if GMASK.use_simple_ground: P = P[simple_ground_mask(P, GMASK.ground_z_margin)]\n",
    "        objs = parse_objects(load_label(Path(m[\"labels_path\"])))\n",
    "        if not objs: continue\n",
    "\n",
    "        y_true = gt_assign_for_eval(P, objs)\n",
    "        asg = assign_two_stage(model, P, m[\"frame_idx\"], seeds_xy, seeds_emb, mahal_state)\n",
    "\n",
    "        # births\n",
    "        if ECFG.allow_births:\n",
    "            XY = P[:, :2].astype(np.float32)\n",
    "            un = np.where(asg < 0)[0]\n",
    "            if un.size >= ECFG.birth_min_pts:\n",
    "                rho = knn_density_xy(XY)\n",
    "                med = float(np.median(rho)) if rho.size else 0.0\n",
    "                keep_un = un[rho[un] >= (ECFG.birth_keep_if_dense_q * (med + 1e-6))]\n",
    "                if keep_un.size >= ECFG.birth_min_pts:\n",
    "                    # quick greedy clustering in XY\n",
    "                    used = np.zeros(keep_un.size, bool); new_xy, new_emb = [], []\n",
    "                    for ii in range(keep_un.size):\n",
    "                        if used[ii]: continue\n",
    "                        p = XY[keep_un[ii]]\n",
    "                        d = np.linalg.norm(XY[keep_un] - p[None, :], axis=1)\n",
    "                        grp = np.where((d <= ECFG.birth_eps_xy) & (~used))[0]\n",
    "                        if grp.size >= ECFG.birth_min_pts:\n",
    "                            ctr = np.median(XY[keep_un][grp], axis=0).astype(np.float32)\n",
    "                            if seeds_xy.size:\n",
    "                                dmin = np.min(np.linalg.norm(seeds_xy - ctr[None, :], axis=1))\n",
    "                                if dmin < ECFG.birth_min_d_from_seeds:\n",
    "                                    used[grp] = True; continue\n",
    "                            cl = pca_tighten_eval(P[keep_un][grp])\n",
    "                            if cl.shape[0] >= ECFG.birth_min_pts:\n",
    "                                new_xy.append(ctr); new_emb.append(seed_proto(model, cl, m[\"frame_idx\"]))\n",
    "                            used[grp] = True\n",
    "                    if new_xy:\n",
    "                        seeds_xy = np.concatenate([seeds_xy, np.stack(new_xy, 0)], 0)\n",
    "                        seeds_emb = np.concatenate([seeds_emb, np.stack(new_emb, 0)], 0)\n",
    "                        miss_counts.extend([0] * len(new_xy))\n",
    "\n",
    "        # deaths\n",
    "        if ECFG.allow_deaths and seeds_xy.shape[0]:\n",
    "            had_hit = [np.any(asg == k) for k in range(seeds_xy.shape[0])]\n",
    "            for k, hit in enumerate(had_hit): miss_counts[k] = 0 if hit else (miss_counts[k] + 1)\n",
    "            keep = np.array([c <= ECFG.death_max_misses for c in miss_counts], bool)\n",
    "            if not np.all(keep):\n",
    "                seeds_xy, seeds_emb = seeds_xy[keep], seeds_emb[keep]\n",
    "                miss_counts = [c for c, k in zip(miss_counts, keep) if k]\n",
    "                if mahal_state[\"mu\"] is not None:\n",
    "                    mahal_state[\"mu\"] = mahal_state[\"mu\"][keep]\n",
    "                    mahal_state[\"covinv\"] = mahal_state[\"covinv\"][keep]\n",
    "\n",
    "        # map predicted cluster IDs to GT IDs for pointwise metrics\n",
    "        pred_ids = np.unique(asg[asg >= 0]).astype(int)\n",
    "        if pred_ids.size and objs:\n",
    "            pred_centers = []\n",
    "            for pid in pred_ids:\n",
    "                pts = P[asg == pid, :2]\n",
    "                pred_centers.append(np.median(pts, axis=0).astype(np.float32) if pts.size else np.array([np.nan, np.nan], np.float32))\n",
    "            pred_centers = np.stack(pred_centers, 0)\n",
    "            gt_tids = np.array([int(o[\"track_id\"]) for o in objs], np.int64)\n",
    "            gt_centers = np.stack([o[\"pos\"][:2] for o in objs], 0).astype(np.float32)\n",
    "            D = np.linalg.norm(pred_centers[:, None, :] - gt_centers[None, :, :], axis=2)\n",
    "            D[~np.isfinite(D)] = np.inf\n",
    "            mapping = {}\n",
    "            while np.isfinite(D).any():\n",
    "                flat = np.nanargmin(D); i, j = divmod(flat, D.shape[1])\n",
    "                if not np.isfinite(D[i, j]): break\n",
    "                mapping[int(pred_ids[i])] = int(gt_tids[j])\n",
    "                D[i, :] = np.inf; D[:, j] = np.inf\n",
    "            y_pred = np.array([mapping.get(int(k), -1) if k >= 0 else -1 for k in asg], np.int64)\n",
    "        else:\n",
    "            y_pred = -np.ones_like(asg, np.int64)\n",
    "\n",
    "        accs.append(point_accuracy(y_true, y_pred))\n",
    "        _, _, F1 = bcubed_scores(y_true, y_pred); f1s.append(F1)\n",
    "\n",
    "    return {\"PointAcc\": float(np.mean(accs) if accs else 0.0),\n",
    "            \"B3F1\":    float(np.mean(f1s) if f1s else 0.0)}\n",
    "# ----------------------------- End -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392d930-7848-4e65-8731-1a5aeb1fd1b6",
   "metadata": {},
   "source": [
    "# Fine-Grained Classification Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a6479-bc00-4cd2-8a32-e5235fbf768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-grained classifier on exported crops.\n",
    "Pipeline: (1) build per-sequence CSVs -> (2) train per-frame DGCNN\n",
    "-> (3) cache per-frame logits -> (4) train tiny temporal head\n",
    "+ rich metrics and fixed-horizon / dynamic-stopping reports.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, math, time, re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ----------------------------- Paths & Config -----------------------------\n",
    "try:\n",
    "    FG_ROOT = paths.FG_EXPORT          # from the tracking script if in same session\n",
    "    CKPT_DIR = paths.CKPT_DIR\n",
    "except NameError:\n",
    "    FG_ROOT  = Path(r\"C:\\UNI\\Thesis\\Dog Data\\fg_export\")\n",
    "    CKPT_DIR = Path(r\"C:\\UNI\\Thesis\\Dog Data\\checkpoints\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CLS_NAMES = [\"dog\", \"human\", \"atlas\"]\n",
    "CLS_TO_ID = {c: i for i, c in enumerate(CLS_NAMES)}\n",
    "\n",
    "# Per-frame graph\n",
    "K_FOLDS = 5; KNN_K = 16; EPOCHS = 12; BATCH = 24; LR = 1e-3\n",
    "STEP = 6; GAMMA = 0.5; CLIP = 1.0; AMP = True; MAX_PTS_PER_CROP = 2048\n",
    "\n",
    "# Temporal head & eval\n",
    "T_EPOCHS = 25; T_BATCH = 16; T_LR = 1e-3; T_STEP = 12; T_GAMMA = 0.5; T_CLIP = 1.0\n",
    "MAX_T = 64; DO_FULL_TEMPORAL_EVAL = False\n",
    "\n",
    "# Geometry breakdown\n",
    "RANGE_BINS_METERS = [0, 20, 45, 95]\n",
    "CENTER_ANG_DEG = 7.0\n",
    "\n",
    "# Dynamic-stopping knobs\n",
    "DECISION_THRESH = 0.60\n",
    "STABLE_K        = 1\n",
    "DYN_MAX_FRAMES  = 8\n",
    "\n",
    "# ----------------------------- I/O Helpers -----------------------------\n",
    "def _resolve(p: str, base: Path) -> Path:\n",
    "    \"\"\"Resolve relative path inside FG export trees.\"\"\"\n",
    "    pp = Path(p)\n",
    "    if pp.is_absolute() and pp.exists(): return pp\n",
    "    if (base/pp).exists(): return base/pp\n",
    "    hits = list(base.rglob(pp.name))\n",
    "    return hits[0] if hits else (base/pp)\n",
    "\n",
    "def load_xyz_from_crop(npz_path: Path) -> np.ndarray:\n",
    "    \"\"\"Return centered XYZI (I=0), subsampled.\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    if \"xyz\" in d:\n",
    "        X = np.asarray(d[\"xyz\"], np.float32)\n",
    "    else:\n",
    "        for k in (\"points\",\"xyzi\",\"pc\",\"pts\",\"arr_0\"):\n",
    "            if k in d and isinstance(d[k], np.ndarray):\n",
    "                arr = d[k]; break\n",
    "        else:\n",
    "            return np.zeros((1,4), np.float32)\n",
    "        X = arr[:, :3].astype(np.float32) if arr.ndim==2 and arr.shape[1]>=3 else np.zeros((1,3), np.float32)\n",
    "    if X.shape[0] == 0: X = np.zeros((1,3), np.float32)\n",
    "    if X.shape[0] > MAX_PTS_PER_CROP:\n",
    "        idx = np.random.choice(X.shape[0], MAX_PTS_PER_CROP, replace=False); X = X[idx]\n",
    "    X = X - X.mean(0, keepdims=True)\n",
    "    return np.concatenate([X, np.zeros((X.shape[0],1), np.float32)], 1)\n",
    "\n",
    "def load_raw_xyz_for_pose(npz_path: Path) -> np.ndarray:\n",
    "    \"\"\"Raw XYZ for pose (range/bearing) estimation.\"\"\"\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    if \"xyz\" in d:\n",
    "        return np.asarray(d[\"xyz\"], np.float32)\n",
    "    for k in (\"points\",\"xyzi\",\"pc\",\"pts\",\"arr_0\"):\n",
    "        if k in d and isinstance(d[k], np.ndarray):\n",
    "            arr = d[k]; return arr[:, :3].astype(np.float32) if arr.ndim==2 and arr.shape[1]>=3 else np.zeros((0,3), np.float32)\n",
    "    return np.zeros((0,3), np.float32)\n",
    "\n",
    "# ----------------------------- Sequence CSVs -----------------------------\n",
    "def build_sequence_csvs_from_exports(fold_idx: int) -> Tuple[Path, Path, Path]:\n",
    "    \"\"\"From fg_export manifests -> per-sequence CSVs (train/val).\"\"\"\n",
    "    outdir = FG_ROOT / \"clf_outputs\"; (outdir / \"seq_logits\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _load(split):\n",
    "        m = FG_ROOT / split / f\"fold_{fold_idx}\" / \"manifest.csv\"\n",
    "        df = pd.read_csv(m)\n",
    "        df[\"label\"] = df[\"class\"].map(lambda c: CLS_TO_ID.get(str(c).lower(), -1))\n",
    "        df = df[df[\"label\"] >= 0].copy()\n",
    "        df[\"abs_npz\"] = df[\"rel_path\"].apply(lambda r: str(_resolve(r, m.parent)))\n",
    "        return df\n",
    "\n",
    "    def _to_sequences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for (b, tid, cls), g in df.groupby([\"bucket\",\"track_id\",\"class\"], sort=False):\n",
    "            g2 = g.sort_values(\"frame_idx\")\n",
    "            rows.append({\n",
    "                \"seq_id\": f\"{b}__t{int(tid)}\",\n",
    "                \"bucket\": b,\n",
    "                \"track_id\": int(tid),\n",
    "                \"roi_npz_paths\": \"|\".join(g2[\"abs_npz\"].tolist()),\n",
    "                \"class\": cls,\n",
    "                \"label\": int(CLS_TO_ID.get(str(cls).lower(), -1)),\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    sq_tr = _to_sequences(_load(\"train\")); sq_va = _to_sequences(_load(\"val\"))\n",
    "    csv_tr = outdir / f\"sequences_train_fold{fold_idx}.csv\"\n",
    "    csv_va = outdir / f\"sequences_val_fold{fold_idx}.csv\"\n",
    "    sq_tr.to_csv(csv_tr, index=False); sq_va.to_csv(csv_va, index=False)\n",
    "\n",
    "    folds_json = outdir / \"folds.json\"\n",
    "    js = {\"n_splits\": K_FOLDS, \"folds\": []}\n",
    "    for k in range(1, K_FOLDS+1):\n",
    "        js[\"folds\"].append({\"train\": sq_tr[\"seq_id\"].tolist() if k==fold_idx else [],\n",
    "                            \"val\":   sq_va[\"seq_id\"].tolist() if k==fold_idx else []})\n",
    "    folds_json.write_text(json.dumps(js, indent=2), encoding=\"utf-8\")\n",
    "    return outdir, csv_tr, csv_va\n",
    "\n",
    "# ----------------------------- DGCNN (Var-N) -----------------------------\n",
    "def knn_graph_masked(x, mask, k):\n",
    "    \"\"\"KNN indices respecting variable point counts per batch item.\"\"\"\n",
    "    B,N,F = x.shape; device = x.device\n",
    "    idx_out = torch.zeros(B,N,k, dtype=torch.long, device=device)\n",
    "    vc = mask.sum(1)\n",
    "    for b in range(B):\n",
    "        n = int(vc[b])\n",
    "        if n <= 1:\n",
    "            inds = torch.nonzero(mask[b], as_tuple=False).squeeze(1)\n",
    "            fill = int(inds[0].item()) if n==1 else 0\n",
    "            idx_out[b].fill_(fill); continue\n",
    "        inds = torch.nonzero(mask[b], as_tuple=False).squeeze(1); xb = x[b, inds, :]\n",
    "        try:\n",
    "            with torch.amp.autocast('cuda', enabled=x.is_cuda): d = torch.cdist(xb, xb)\n",
    "        except RuntimeError:\n",
    "            d = torch.cdist(xb.cpu(), xb.cpu()).to(device)\n",
    "        d.fill_diagonal_(float('inf'))\n",
    "        k_eff = min(k, max(1, n-1))\n",
    "        nbr_local = d.topk(k_eff, dim=1, largest=False).indices\n",
    "        if k_eff < k: nbr_local = torch.cat([nbr_local, nbr_local[:, :1].expand(n, k-k_eff)], 1)\n",
    "        idx_out[b, inds, :] = inds[nbr_local]\n",
    "        if (~mask[b]).any(): idx_out[b, ~mask[b], :] = inds[0]\n",
    "    return idx_out\n",
    "\n",
    "def _gather_neighbors_flat(x, idx):\n",
    "    B,N,F = x.shape; k = idx.size(-1)\n",
    "    base = torch.arange(B, device=x.device).view(B,1,1)*N\n",
    "    return x.reshape(B*N, F)[(idx + base).reshape(-1), :].reshape(B,N,k,F)\n",
    "\n",
    "def masked_max_mean(f, mask):\n",
    "    \"\"\"Global max + mean pooling under mask.\"\"\"\n",
    "    maskC = mask.unsqueeze(-1); f_masked = f.masked_fill(~maskC, float('-inf'))\n",
    "    f_max = torch.where(torch.isfinite(torch.amax(f_masked, dim=1)), torch.amax(f_masked, dim=1), torch.zeros_like(f_masked[:,0,:]))\n",
    "    s = (f * maskC.float()).sum(1); cnt = mask.float().sum(1, keepdim=True).clamp_min(1.0)\n",
    "    return f_max, s / cnt\n",
    "\n",
    "class EdgeConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(nn.Linear(in_ch*2, out_ch, bias=False),\n",
    "                                 nn.BatchNorm1d(out_ch),\n",
    "                                 nn.LeakyReLU(0.2, inplace=False))\n",
    "    def forward(self, x, mask, idx):\n",
    "        B,N,F = x.shape; k = idx.size(-1)\n",
    "        xi = x.unsqueeze(2).expand(-1,-1,k,-1); xj = _gather_neighbors_flat(x, idx)\n",
    "        e = torch.cat([xi, xj - xi], -1).reshape(B*N*k, 2*F)\n",
    "        f = self.mlp(e).view(B,N,k,-1).max(2)[0] * mask.unsqueeze(-1).float()\n",
    "        return f\n",
    "\n",
    "class DGCNNVarN(nn.Module):\n",
    "    def __init__(self, in_ch=4, k=16, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.ec1 = EdgeConv(in_ch, 64); self.ec2 = EdgeConv(64, 128); self.ec3 = EdgeConv(128, 256)\n",
    "        glob = (64+128+256)*2\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(glob, 256, bias=False), nn.BatchNorm1d(256), nn.LeakyReLU(0.2, inplace=False), nn.Dropout(0.3),\n",
    "            nn.Linear(256, 256, bias=False), nn.BatchNorm1d(256), nn.LeakyReLU(0.2, inplace=False), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "    def forward(self, x, mask):\n",
    "        idx = knn_graph_masked(x, mask, self.k)\n",
    "        f1 = self.ec1(x, mask, idx); f2 = self.ec2(f1, mask, idx); f3 = self.ec3(f2, mask, idx)\n",
    "        gmax, gmean = masked_max_mean(torch.cat([f1,f2,f3], -1), mask)\n",
    "        return self.head(torch.cat([gmax, gmean], -1))\n",
    "\n",
    "# ----------------------------- Datasets / Collates -----------------------------\n",
    "class CropsFrameDS(Dataset):\n",
    "    def __init__(self, manifest_csv: Path):\n",
    "        df = pd.read_csv(manifest_csv)\n",
    "        df[\"label\"] = df[\"class\"].map(lambda c: CLS_TO_ID.get(str(c).lower(), -1))\n",
    "        df = df[df[\"label\"] >= 0].copy()\n",
    "        df[\"abs_npz\"] = df[\"rel_path\"].apply(lambda r: str(_resolve(r, manifest_csv.parent)))\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        X = load_xyz_from_crop(Path(r[\"abs_npz\"]))\n",
    "        return torch.from_numpy(X), torch.tensor(int(r[\"label\"]), dtype=torch.long)\n",
    "\n",
    "def collate_frames(batch):\n",
    "    Ns = [x[0].shape[0] for x in batch]; Nmax = max(Ns) if Ns else 1; B = len(batch)\n",
    "    X = torch.zeros(B, Nmax, 4, dtype=torch.float32); M = torch.zeros(B, Nmax, dtype=torch.bool); Y = torch.zeros(B, dtype=torch.long)\n",
    "    for b,(Xi,y) in enumerate(batch): n = Xi.shape[0]; X[b,:n,:] = Xi; M[b,:n] = True; Y[b] = y\n",
    "    return X, M, Y\n",
    "\n",
    "class CropsFrameEvalDS(CropsFrameDS):\n",
    "    def __getitem__(self, i):\n",
    "        Xc, y = super().__getitem__(i); r = self.df.iloc[i]; Xraw = load_raw_xyz_for_pose(Path(r[\"abs_npz\"]))\n",
    "        if Xraw.shape[0]==0: rng, th = 0.0, 0.0\n",
    "        else:\n",
    "            med = np.median(Xraw, 0).astype(np.float32); rng = float(np.hypot(med[0], med[1])); th = float(np.arctan2(med[1], med[0]))\n",
    "        return Xc, y, torch.tensor(rng, np.float32), torch.tensor(th, np.float32)\n",
    "\n",
    "def collate_frames_with_pose(batch):\n",
    "    X,M,Y = collate_frames([(b[0], b[1]) for b in batch])\n",
    "    B = len(batch); R = torch.zeros(B, dtype=torch.float32); TH = torch.zeros(B, dtype=torch.float32)\n",
    "    for i,(_,_,r,th) in enumerate(batch): R[i] = r; TH[i] = th\n",
    "    return X,M,Y,R,TH\n",
    "\n",
    "# Cached-logits datasets\n",
    "class LogitSeqDS(Dataset):\n",
    "    def __init__(self, outdir: Path, split: str, fold_idx: int):\n",
    "        self.df = pd.read_csv(outdir / f\"sequences_{split}_fold{fold_idx}.csv\")\n",
    "        self.dir = outdir / \"seq_logits\"\n",
    "        self.files = [(self.dir / f\"{str(r['seq_id'])}.npz\", int(r[\"label\"])) for _, r in self.df.iterrows()\n",
    "                      if (self.dir / f\"{str(r['seq_id'])}.npz\").exists()]\n",
    "        if not self.files: raise RuntimeError(f\"No cached sequences for fold {fold_idx} {split}.\")\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, i):\n",
    "        p, y = self.files[i]; L = np.load(p)[\"logits\"].astype(np.float32)\n",
    "        if MAX_T and L.shape[0] > MAX_T:\n",
    "            idx = np.linspace(0, L.shape[0]-1, MAX_T).round().astype(int); L = L[idx]\n",
    "        return torch.from_numpy(L), torch.tensor(y, torch.long)\n",
    "\n",
    "def collate_logits(batch):\n",
    "    xs, ys = zip(*batch); Tm = max(x.shape[0] for x in xs); C = xs[0].shape[1]; B = len(xs)\n",
    "    X = torch.zeros(B, Tm, C, dtype=torch.float32); M = torch.zeros(B, Tm, dtype=torch.bool)\n",
    "    for b,x in enumerate(xs): t = x.shape[0]; X[b,:t,:] = x; M[b,:t] = True\n",
    "    return X, M, torch.stack(ys, 0)\n",
    "\n",
    "# ----------------------------- Train / Cache (Per-frame) -----------------------------\n",
    "@torch.no_grad()\n",
    "def eval_frame(model, dl):\n",
    "    model.eval(); crit = nn.CrossEntropyLoss(); tot=0.0; n=0; acc=0.0\n",
    "    ys=[]; ps=[]\n",
    "    for X,M,Y in dl:\n",
    "        X,M,Y = X.to(DEVICE), M.to(DEVICE), Y.to(DEVICE)\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE.type=='cuda' and AMP)):\n",
    "            logits = model(X,M); loss = crit(logits, Y)\n",
    "        tot += loss.item()*Y.size(0); n += Y.size(0)\n",
    "        ys.append(Y.cpu().numpy()); ps.append(logits.argmax(1).cpu().numpy())\n",
    "    y_true = np.concatenate(ys) if ys else np.array([], int); y_pred = np.concatenate(ps) if ps else np.array([], int)\n",
    "    if len(y_true): acc = float((y_true == y_pred).mean())\n",
    "    return tot/max(1,n), acc\n",
    "\n",
    "def train_per_frame_for_fold(fold_idx: int, outdir: Path) -> Path:\n",
    "    man_tr = FG_ROOT / \"train\" / f\"fold_{fold_idx}\" / \"manifest.csv\"\n",
    "    man_va = FG_ROOT / \"val\"   / f\"fold_{fold_idx}\" / \"manifest.csv\"\n",
    "    dl_tr = DataLoader(CropsFrameDS(man_tr), batch_size=BATCH, shuffle=True,  num_workers=0,\n",
    "                       collate_fn=collate_frames, pin_memory=(DEVICE.type=='cuda'))\n",
    "    dl_va = DataLoader(CropsFrameDS(man_va), batch_size=BATCH, shuffle=False, num_workers=0,\n",
    "                       collate_fn=collate_frames, pin_memory=(DEVICE.type=='cuda'))\n",
    "\n",
    "    model = DGCNNVarN(in_ch=4, k=KNN_K, num_classes=len(CLS_NAMES)).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    sch = torch.optim.lr_scheduler.StepLR(opt, step_size=STEP, gamma=GAMMA)\n",
    "    scaler = torch.amp.GradScaler('cuda') if (DEVICE.type=='cuda' and AMP) else None\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_vloss, best_state, best_ep = float('inf'), None, -1\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); run=0.0; cnt=0; t0=time.perf_counter()\n",
    "        for X,M,Y in dl_tr:\n",
    "            X,M,Y = X.to(DEVICE), M.to(DEVICE), Y.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            if scaler is None:\n",
    "                logits = model(X,M); loss = crit(logits, Y)\n",
    "                loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), CLIP); opt.step()\n",
    "            else:\n",
    "                with torch.amp.autocast('cuda'): logits = model(X,M); loss = crit(logits, Y)\n",
    "                scaler.scale(loss).backward(); scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "                scaler.step(opt); scaler.update()\n",
    "            run += loss.item()*Y.size(0); cnt += Y.size(0)\n",
    "        sch.step()\n",
    "        vl, va = eval_frame(model, dl_va)\n",
    "        if vl < best_vloss:\n",
    "            best_vloss, best_ep = vl, ep\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "        dt=time.perf_counter()-t0\n",
    "        print(f\"[frame fold {fold_idx:02d} ep {ep:02d}] train={run/max(1,cnt):.4f}  val={vl:.4f}  acc={100*va:.1f}%  ({dt:.1f}s)\")\n",
    "\n",
    "    ck = outdir / f\"per_frame_graph_best_vloss_fold{fold_idx}.pt\"\n",
    "    torch.save({\"state_dict\": best_state, \"classes\": CLS_NAMES}, ck)\n",
    "    print(f\"[frame fold {fold_idx}] best_val_loss={best_vloss:.4f} @ ep {best_ep} -> {ck}\")\n",
    "    return ck\n",
    "\n",
    "def load_per_frame_model(ckpt_path: Path) -> nn.Module:\n",
    "    ck = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    state = ck[\"state_dict\"] if isinstance(ck, dict) and \"state_dict\" in ck else ck\n",
    "    model = DGCNNVarN(in_ch=4, k=KNN_K, num_classes=len(CLS_NAMES)).to(DEVICE)\n",
    "    model.load_state_dict(state); model.eval()\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def cache_sequence_logits_for_fold(fold_idx: int, outdir: Path, ck_graph: Path):\n",
    "    \"\"\"Run per-frame model over sequences; save per-sequence logits (TxC) to NPZ.\"\"\"\n",
    "    seq_dir = outdir / \"seq_logits\"; seq_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_tr = pd.read_csv(outdir / f\"sequences_train_fold{fold_idx}.csv\")\n",
    "    df_va = pd.read_csv(outdir / f\"sequences_val_fold{fold_idx}.csv\")\n",
    "    model = load_per_frame_model(ck_graph)\n",
    "\n",
    "    def _cache(df, tag):\n",
    "        for i, r in df.iterrows():\n",
    "            L = []\n",
    "            for p in str(r[\"roi_npz_paths\"]).split(\"|\"):\n",
    "                Xi = load_xyz_from_crop(Path(p))\n",
    "                with torch.amp.autocast('cuda', enabled=(DEVICE.type=='cuda' and AMP)):\n",
    "                    X = torch.from_numpy(Xi[None]).float().to(DEVICE)\n",
    "                    M = torch.ones(1, Xi.shape[0], dtype=torch.bool, device=DEVICE)\n",
    "                    logits = model(X, M)\n",
    "                L.append(logits.squeeze(0).detach().cpu().numpy().astype(np.float32))\n",
    "            L = np.stack(L, 0) if L else np.zeros((1, len(CLS_NAMES)), np.float32)\n",
    "            np.savez_compressed(seq_dir / f\"{str(r['seq_id'])}.npz\", logits=L, label=np.int64(int(r[\"label\"])))\n",
    "            if (i % 25) == 0: print(f\"[cache fold{fold_idx} {tag}] {i}/{len(df)} {str(r['seq_id'])}  T={L.shape[0]}\")\n",
    "        print(f\"[cache] fold{fold_idx} {tag}: {len(df)} sequences processed\")\n",
    "\n",
    "    _cache(df_tr, \"train\"); _cache(df_va, \"val\")\n",
    "\n",
    "# ----------------------------- Temporal Head -----------------------------\n",
    "class TinyTemporalHead(nn.Module):\n",
    "    \"\"\"Tiny conv+attn across cached logits (TxC) -> class.\"\"\"\n",
    "    def __init__(self, C, hid=64, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(C, hid); self.conv = nn.Conv1d(hid, hid, 3, padding=1)\n",
    "        self.attn_v = nn.Linear(hid, 1, bias=False)\n",
    "        self.fc = nn.Sequential(nn.Linear(hid,128), nn.ReLU(True), nn.Dropout(0.3), nn.Linear(128, num_classes))\n",
    "    def forward(self, X, M):\n",
    "        B,T,C = X.shape\n",
    "        H = self.proj(X); Hc = self.conv(H.transpose(1,2)).transpose(1,2)\n",
    "        H = (H+Hc) * M.unsqueeze(-1).to(H.dtype)\n",
    "        s = self.attn_v(torch.tanh(H)).squeeze(-1)\n",
    "        s = s.masked_fill(~M, torch.tensor(-1e4, dtype=s.dtype, device=s.device))\n",
    "        w = torch.softmax(s - s.max(1, keepdim=True).values, 1)\n",
    "        return self.fc((H * w.unsqueeze(-1)).sum(1))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_temporal(model, dl, crit):\n",
    "    model.eval(); tot=0.0; n=0; acc=0.0\n",
    "    for X,M,Y in dl:\n",
    "        X,M,Y = X.to(DEVICE), M.to(DEVICE), Y.to(DEVICE)\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE.type=='cuda' and AMP)):\n",
    "            logits = model(X,M); loss = crit(logits, Y)\n",
    "        tot += loss.item()*Y.size(0); n += Y.size(0); acc += (logits.argmax(1)==Y).sum().item()\n",
    "    return tot/max(1,n), acc/max(1,n)\n",
    "\n",
    "def train_temporal_for_fold(fold_idx: int, outdir: Path) -> Path:\n",
    "    tr = LogitSeqDS(outdir, \"train\", fold_idx); va = LogitSeqDS(outdir, \"val\", fold_idx)\n",
    "    dl_tr = DataLoader(tr, batch_size=T_BATCH, shuffle=True,  num_workers=0, collate_fn=collate_logits, pin_memory=(DEVICE.type=='cuda'))\n",
    "    dl_va = DataLoader(va, batch_size=T_BATCH, shuffle=False, num_workers=0, collate_fn=collate_logits, pin_memory=(DEVICE.type=='cuda'))\n",
    "\n",
    "    model = TinyTemporalHead(C=len(CLS_NAMES), hid=64, num_classes=len(CLS_NAMES)).to(DEVICE)\n",
    "    crit = nn.CrossEntropyLoss(); opt = torch.optim.Adam(model.parameters(), lr=T_LR)\n",
    "    sch = torch.optim.lr_scheduler.StepLR(opt, step_size=T_STEP, gamma=T_GAMMA)\n",
    "    scaler = torch.amp.GradScaler('cuda') if (DEVICE.type=='cuda' and AMP) else None\n",
    "\n",
    "    best_vloss, best_state, best_ep = float('inf'), None, -1\n",
    "    for ep in range(1, T_EPOCHS+1):\n",
    "        model.train(); run=0.0; cnt=0\n",
    "        for X,M,Y in dl_tr:\n",
    "            X,M,Y = X.to(DEVICE), M.to(DEVICE), Y.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            if scaler is None:\n",
    "                logits = model(X,M); loss = crit(logits, Y); loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), T_CLIP); opt.step()\n",
    "            else:\n",
    "                with torch.amp.autocast('cuda'): logits = model(X,M); loss = crit(logits, Y)\n",
    "                scaler.scale(loss).backward(); scaler.unscale_(opt); nn.utils.clip_grad_norm_(model.parameters(), T_CLIP)\n",
    "                scaler.step(opt); scaler.update()\n",
    "            run += loss.item()*Y.size(0); cnt += Y.size(0)\n",
    "        sch.step()\n",
    "        vl, va = evaluate_temporal(model, dl_va, crit)\n",
    "        if vl < best_vloss: best_vloss, best_ep, best_state = vl, ep, {k: v.detach().cpu() for k,v in model.state_dict().items()}\n",
    "        print(f\"[temp fold {fold_idx:02d} ep {ep:02d}] train={run/max(1,cnt):.4f}  val={vl:.4f}  acc={100*va:.1f}%\")\n",
    "\n",
    "    ck = outdir / f\"temporal_from_logits_best_vloss_fold{fold_idx}.pt\"\n",
    "    torch.save({\"state_dict\": best_state, \"classes\": CLS_NAMES}, ck)\n",
    "    print(f\"[temp fold {fold_idx}] best_val_loss={best_vloss:.4f} @ ep {best_ep} -> {ck}\")\n",
    "    return ck\n",
    "\n",
    "# ----------------------------- Metrics (NumPy, no sklearn) -----------------------------\n",
    "def _softmax_np(z):\n",
    "    z = z - np.max(z, 1, keepdims=True); e = np.exp(z); return e / np.clip(e.sum(1, keepdims=True), 1e-9, None)\n",
    "\n",
    "def confusion_matrix_np(y_true, y_pred, nC):\n",
    "    cm = np.zeros((nC,nC), np.int64)\n",
    "    for t,p in zip(y_true, y_pred):\n",
    "        if 0 <= t < nC and 0 <= p < nC: cm[t,p] += 1\n",
    "    return cm\n",
    "\n",
    "def precision_recall_f1_from_cm(cm):\n",
    "    tp = np.diag(cm).astype(np.float64); fp = cm.sum(0) - tp; fn = cm.sum(1) - tp\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        prec = np.where(tp+fp>0, tp/(tp+fp), 0.0); rec = np.where(tp+fn>0, tp/(tp+fn), 0.0)\n",
    "        f1 = np.where(prec+rec>0, 2*prec*rec/(prec+rec), 0.0)\n",
    "    return prec, rec, f1, cm.sum(1).astype(int)\n",
    "\n",
    "def top_k_accuracy(prob, y_true, k=3):\n",
    "    k = min(k, prob.shape[1]); topk = np.argsort(-prob, 1)[:, :k]\n",
    "    return float(np.any(topk == y_true[:,None], 1).mean()) if len(y_true) else 0.0\n",
    "\n",
    "def brier_score(prob, y_true, nC):\n",
    "    oh = np.zeros_like(prob); oh[np.arange(len(y_true)), y_true] = 1.0\n",
    "    return float(np.mean(np.sum((prob - oh)**2, 1))) if len(y_true) else 0.0\n",
    "\n",
    "def ece_score(prob, y_true, n_bins=15):\n",
    "    conf = prob.max(1); preds = prob.argmax(1); acc = (preds == y_true).astype(np.float64)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1); ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        m = (conf >= bins[i]) & (conf < bins[i+1]) if i < n_bins-1 else (conf >= bins[i]) & (conf <= bins[i+1])\n",
    "        if not np.any(m): continue\n",
    "        ece += m.mean() * abs(acc[m].mean() - conf[m].mean())\n",
    "    return float(ece)\n",
    "\n",
    "def _binary_auc(scores, labels):\n",
    "    if len(scores)==0: return 0.0\n",
    "    o = np.argsort(-scores); y = labels[o]; tp = np.cumsum(y); fp = np.cumsum(1-y)\n",
    "    tpr = tp / max(1, tp[-1]); fpr = fp / max(1, fp[-1])\n",
    "    tpr = np.concatenate([[0.0], tpr, [1.0]]); fpr = np.concatenate([[0.0], fpr, [1.0]])\n",
    "    return float(np.trapezoid(tpr, fpr))\n",
    "\n",
    "def _binary_auprc(scores, labels):\n",
    "    if len(scores)==0: return 0.0\n",
    "    o = np.argsort(-scores); y = labels[o]; tp = np.cumsum(y); fp = np.cumsum(1-y)\n",
    "    prec = tp / np.maximum(1, tp+fp); rec = tp / max(1, tp[-1])\n",
    "    rec = np.concatenate([[0.0], rec]); prec = np.concatenate([[float(np.mean(labels)) if len(labels) else 0.0], prec])\n",
    "    return float(np.trapezoid(prec, rec))\n",
    "\n",
    "def auroc_auprc_ovr(prob, y_true, nC):\n",
    "    auroc = np.zeros(nC, np.float64); auprc = np.zeros(nC, np.float64)\n",
    "    for c in range(nC):\n",
    "        s = prob[:, c]; lab = (y_true == c).astype(np.int32)\n",
    "        auroc[c] = _binary_auc(s, lab); auprc[c] = _binary_auprc(s, lab)\n",
    "    return auroc, auprc\n",
    "\n",
    "def print_confusion(cm, names):\n",
    "    print(\"Confusion (rows=GT, cols=Pred):\")\n",
    "    head = \"      \" + \" \".join([f\"{c[:6]:>7s}\" for c in names]); print(head)\n",
    "    for i,row in enumerate(cm): print(f\"{names[i][:6]:>6s} \" + \" \".join([f\"{v:7d}\" for v in row]))\n",
    "\n",
    "def print_metric_block(title, y_true, y_pred, prob, names):\n",
    "    nC = len(names); print(f\"\\n===== {title} =====\")\n",
    "    if len(y_true)==0: print(\"No samples.\"); return\n",
    "    acc = float((y_true == y_pred).mean()); cm = confusion_matrix_np(y_true, y_pred, nC)\n",
    "    prec, rec, f1, sup = precision_recall_f1_from_cm(cm); auroc, auprc = auroc_auprc_ovr(prob, y_true, nC)\n",
    "    print_confusion(cm, names)\n",
    "    print(\"\\nPer-class:\\n class      supp   prec    rec     f1    AUROC  AUPRC\")\n",
    "    for i,c in enumerate(names):\n",
    "        print(f\" {c:<9s} {sup[i]:6d}  {prec[i]:6.3f} {rec[i]:6.3f} {f1[i]:6.3f}  {auroc[i]:6.3f} {auprc[i]:6.3f}\")\n",
    "    print(f\"\\nMacro avg : prec={np.mean(prec):.3f} rec={np.mean(rec):.3f} f1={np.mean(f1):.3f}  AUROC={auroc.mean():.3f} AUPRC={auprc.mean():.3f}\")\n",
    "    print(f\"Micro/Acc : acc={acc:.3f}  top-3={top_k_accuracy(prob, y_true, k=min(3,nC)):.3f}\")\n",
    "    print(f\"Calibration: ECE={ece_score(prob, y_true, 15):.3f}  Brier={brier_score(prob, y_true, nC):.3f}\")\n",
    "\n",
    "def print_accuracy_by_range_and_angle(title, y_true, y_pred, ranges_m, thetas_rad, bins_m, center_deg, names):\n",
    "    print(f\"\\n----- {title}: Accuracy by range and angle -----\")\n",
    "    edges = np.array(bins_m, float)\n",
    "    for i in range(len(edges)-1):\n",
    "        lo, hi = edges[i], edges[i+1]; m = (ranges_m >= lo) & (ranges_m < hi)\n",
    "        print(f\"Range [{lo:>4.0f},{hi:>4.0f}) m : {100*float((y_true[m]==y_pred[m]).mean()) if np.any(m) else 0.0:5.1f}%  (n={m.sum()})\")\n",
    "    th_abs = np.abs(thetas_rad) * 180.0 / np.pi\n",
    "    m_center = th_abs <= float(center_deg); m_off = ~m_center\n",
    "    if np.any(m_center):\n",
    "        print(f\"\\nCenter (|bearing| <= {center_deg:.1f} deg): {100*float((y_true[m_center]==y_pred[m_center]).mean()):5.1f}%  (n={m_center.sum()})\")\n",
    "    if np.any(m_off):\n",
    "        print(f\"Off-center (>|bearing|):                 {100*float((y_true[m_off]==y_pred[m_off]).mean()):5.1f}%  (n={m_off.sum()})\")\n",
    "\n",
    "# Collectors that also return pose arrays\n",
    "@torch.no_grad()\n",
    "def collect_frame_outputs_with_pose(model, dl, nC: int):\n",
    "    model.eval(); crit = nn.CrossEntropyLoss(); tot=0.0; n=0\n",
    "    ys=[]; ps=[]; logits_all=[]; rngs=[]; ths=[]\n",
    "    for X,M,Y,R,TH in dl:\n",
    "        X,M,Y = X.to(DEVICE), M.to(DEVICE), Y.to(DEVICE)\n",
    "        with torch.amp.autocast('cuda', enabled=(DEVICE.type=='cuda' and AMP)):\n",
    "            logits = model(X,M); loss = crit(logits, Y)\n",
    "        tot += loss.item()*Y.size(0); n += Y.size(0)\n",
    "        ys.append(Y.cpu().numpy()); logits_all.append(logits.detach().cpu().numpy()); ps.append(logits.argmax(1).cpu().numpy())\n",
    "        rngs.append(R.numpy()); ths.append(TH.numpy())\n",
    "    y_true = np.concatenate(ys) if ys else np.zeros((0,), int)\n",
    "    y_pred = np.concatenate(ps) if ps else np.zeros((0,), int)\n",
    "    logits = np.concatenate(logits_all) if logits_all else np.zeros((0,nC), np.float32)\n",
    "    prob = _softmax_np(logits)\n",
    "    Rall = np.concatenate(rngs) if rngs else np.zeros((0,), np.float32)\n",
    "    THall= np.concatenate(ths)  if ths  else np.zeros((0,), np.float32)\n",
    "    return y_true, y_pred, prob, Rall, THall, (tot/max(1,n))\n",
    "\n",
    "# ----------------------------- Fixed-horizon & Dynamic eval -----------------------------\n",
    "def _load_seq_csv_for_eval(outdir: Path, split: str, fold_idx: int) -> pd.DataFrame:\n",
    "    df = pd.read_csv(outdir / f\"sequences_{split}_fold{fold_idx}.csv\")\n",
    "    if \"label\" not in df.columns:\n",
    "        df[\"label\"] = df[\"class\"].map(lambda c: CLS_TO_ID.get(str(c).lower(), -1))\n",
    "    return df\n",
    "\n",
    "def _seq_paths(row: pd.Series) -> List[str]:\n",
    "    s = str(row[\"roi_npz_paths\"]); return [p for p in s.split(\"|\") if p] if s else []\n",
    "\n",
    "def _pose_for_paths(paths: List[str], t_use: int) -> Tuple[float, float]:\n",
    "    rs, ths = [], []\n",
    "    for q in paths[:t_use]:\n",
    "        Xraw = load_raw_xyz_for_pose(Path(q))\n",
    "        if Xraw.shape[0]==0: continue\n",
    "        med = np.median(Xraw, 0).astype(np.float32); rs.append(float(np.hypot(med[0], med[1]))); ths.append(float(np.arctan2(med[1], med[0])))\n",
    "    return (float(np.median(rs)) if rs else 0.0, float(np.median(ths)) if ths else 0.0)\n",
    "\n",
    "def _fixed_horizon_eval(outdir: Path, fold_idx: int, split: str, horizon: int):\n",
    "    df = _load_seq_csv_for_eval(outdir, split, fold_idx); seq_dir = outdir / \"seq_logits\"\n",
    "    y_true, y_pred, P_all, rngs, ths, missed = [], [], [], [], [], 0\n",
    "    for _, r in df.iterrows():\n",
    "        p = seq_dir / f\"{str(r['seq_id'])}.npz\"\n",
    "        if not p.exists(): continue\n",
    "        L = np.asarray(np.load(p)[\"logits\"], np.float32)\n",
    "        if L.shape[0] < horizon: missed += 1; continue\n",
    "        P = _softmax_np(L)[:horizon].mean(0); pred = int(P.argmax()); y = int(r[\"label\"])\n",
    "        y_true.append(y); y_pred.append(pred); P_all.append(P[None,:])\n",
    "        rng, th = _pose_for_paths(_seq_paths(r), t_use=horizon); rngs.append(rng); ths.append(th)\n",
    "    if not y_true: return None\n",
    "    return np.array(y_true,int), np.array(y_pred,int), np.concatenate(P_all,0), np.array(rngs,np.float32), np.array(ths,np.float32), missed\n",
    "\n",
    "def _dynamic_eval(outdir: Path, fold_idx: int, split: str, conf_thr=DECISION_THRESH, stable_k=STABLE_K, tmax=DYN_MAX_FRAMES):\n",
    "    df = _load_seq_csv_for_eval(outdir, split, fold_idx); seq_dir = outdir / \"seq_logits\"\n",
    "    y_true, y_pred, P_all, rngs, ths, usedT, skipped = [], [], [], [], [], [], 0\n",
    "    for _, r in df.iterrows():\n",
    "        p = seq_dir / f\"{str(r['seq_id'])}.npz\"\n",
    "        if not p.exists(): continue\n",
    "        L = np.asarray(np.load(p)[\"logits\"], np.float32); T = L.shape[0]\n",
    "        if T < 1: skipped += 1; continue\n",
    "        P = _softmax_np(L); last=None; streak=0; decided=False\n",
    "        for t in range(1, min(tmax, T)+1):\n",
    "            P_cum = P[:t].mean(0); pred = int(P_cum.argmax()); conf = float(P_cum[pred])\n",
    "            streak = streak+1 if pred==last else 1; last = pred\n",
    "            if (streak>=stable_k) and (conf>=conf_thr):\n",
    "                rng, th = _pose_for_paths(_seq_paths(r), t_use=t)\n",
    "                y_true.append(int(r[\"label\"])); y_pred.append(pred); P_all.append(P_cum[None,:])\n",
    "                rngs.append(rng); ths.append(th); usedT.append(t); decided=True; break\n",
    "        if not decided: skipped += 1\n",
    "    if not y_true: return None\n",
    "    return np.array(y_true,int), np.array(y_pred,int), np.concatenate(P_all,0), np.array(rngs,np.float32), np.array(ths,np.float32), np.array(usedT,int), skipped\n",
    "\n",
    "def _print_fixed_block(title: str, pack, bins_m=None, center_deg=None):\n",
    "    y_true, y_pred, P, R, TH, missed = pack\n",
    "    print(f\"\\n================ {title} ================\")\n",
    "    print(f\"Used sequences: {len(y_true)}   Skipped (shorter than horizon): {missed}\")\n",
    "    print_metric_block(title, y_true, y_pred, P, CLS_NAMES)\n",
    "    if bins_m is not None: print_accuracy_by_range_and_angle(title, y_true, y_pred, R, TH, bins_m, center_deg, CLS_NAMES)\n",
    "\n",
    "def _print_dynamic_block(title: str, pack, bins_m=None, center_deg=None, fps: Optional[float]=None):\n",
    "    y_true, y_pred, P, R, TH, usedT, skipped = pack\n",
    "    print(f\"\\n================ {title} ================\")\n",
    "    print(f\"Decided sequences: {len(y_true)}   Skipped (no decision <= tmax): {skipped}\")\n",
    "    if len(usedT):\n",
    "        print(f\"Frames-to-decision: mean={usedT.mean():.2f}, median={np.median(usedT):.0f}, p90={np.percentile(usedT,90):.0f}\")\n",
    "        if fps and fps>0: print(f\"In seconds: mean={usedT.mean()/fps:.2f}s, median={np.median(usedT)/fps:.2f}s, p90={np.percentile(usedT,90)/fps:.2f}s\")\n",
    "    print_metric_block(title, y_true, y_pred, P, CLS_NAMES)\n",
    "    if bins_m is not None: print_accuracy_by_range_and_angle(title, y_true, y_pred, R, TH, bins_m, center_deg, CLS_NAMES)\n",
    "\n",
    "# ----------------------------- Orchestrate -----------------------------\n",
    "def run_fine_grained_all_folds():\n",
    "    print(f\"[cfg] FG_ROOT={FG_ROOT}\")\n",
    "    out_all = FG_ROOT / \"clf_outputs\"\n",
    "    AF_y_true=[]; AF_y_pred=[]; AF_prob=[]; AF_rng=[]; AF_th=[]; fold_acc=[]\n",
    "    for fold_idx in range(1, K_FOLDS+1):\n",
    "        print(\"\\n============================\")\n",
    "        print(f\"=== Fine-grained: Fold {fold_idx}/{K_FOLDS} ===\")\n",
    "        print(\"============================\")\n",
    "        outdir, csv_tr, csv_va = build_sequence_csvs_from_exports(fold_idx)\n",
    "        ck_graph = train_per_frame_for_fold(fold_idx, outdir)\n",
    "\n",
    "        # Per-frame VAL metrics + pose breakdown\n",
    "        eval_dl = DataLoader(CropsFrameEvalDS(FG_ROOT / \"val\" / f\"fold_{fold_idx}\" / \"manifest.csv\"),\n",
    "                             batch_size=BATCH, shuffle=False, num_workers=0,\n",
    "                             collate_fn=collate_frames_with_pose, pin_memory=(DEVICE.type=='cuda'))\n",
    "        model_frame = load_per_frame_model(ck_graph)\n",
    "        y_true, y_pred, prob, R, TH, _ = collect_frame_outputs_with_pose(model_frame, eval_dl, nC=len(CLS_NAMES))\n",
    "        if len(y_true):\n",
    "            fold_acc.append(float((y_true==y_pred).mean()))\n",
    "            AF_y_true.append(y_true); AF_y_pred.append(y_pred); AF_prob.append(prob); AF_rng.append(R); AF_th.append(TH)\n",
    "        print_metric_block(f\"Fold {fold_idx} - PER-FRAME (VAL)\", y_true, y_pred, prob, CLS_NAMES)\n",
    "        print_accuracy_by_range_and_angle(f\"Fold {fold_idx} - PER-FRAME (VAL)\", y_true, y_pred, R, TH, RANGE_BINS_METERS, CENTER_ANG_DEG, CLS_NAMES)\n",
    "\n",
    "        # Cache logits and (optionally) train temporal head\n",
    "        cache_sequence_logits_for_fold(fold_idx, outdir, ck_graph)\n",
    "        train_temporal_for_fold(fold_idx, outdir)  # kept for completeness\n",
    "\n",
    "        # Optional whole-sequence temporal eval (disabled by default)\n",
    "        if DO_FULL_TEMPORAL_EVAL:\n",
    "            pass  # can be enabled same as original script\n",
    "\n",
    "    # All-fold summary\n",
    "    if AF_y_true:\n",
    "        Yt = np.concatenate(AF_y_true); Yp = np.concatenate(AF_y_pred); Pb = np.concatenate(AF_prob)\n",
    "        Rg = np.concatenate(AF_rng); Th = np.concatenate(AF_th)\n",
    "        print_metric_block(\"ALL FOLDS - PER-FRAME (VAL)\", Yt, Yp, Pb, CLS_NAMES)\n",
    "        print_accuracy_by_range_and_angle(\"ALL FOLDS - PER-FRAME (VAL)\", Yt, Yp, Rg, Th, RANGE_BINS_METERS, CENTER_ANG_DEG, CLS_NAMES)\n",
    "        print(f\"\\n=== SUMMARY: Per-frame ACC across folds ===\\n{100*np.mean(fold_acc):.2f}% +/- {100*np.std(fold_acc):.2f}%\")\n",
    "\n",
    "# Fixed horizons (t=3/4/5) + Dynamic stopping reports\n",
    "def run_fixed_horizon_and_dynamic_reports(fold_idx: int, split: str = \"val\", fps_for_seconds: Optional[float] = None):\n",
    "    outdir = FG_ROOT / \"clf_outputs\"\n",
    "    for t in (3,4,5):\n",
    "        pack = _fixed_horizon_eval(outdir, fold_idx, split, t)\n",
    "        if pack is not None: _print_fixed_block(f\"FOLD {fold_idx} | {split.upper()} | FIXED HORIZON t={t}\", pack, RANGE_BINS_METERS, CENTER_ANG_DEG)\n",
    "        else: print(f\"[fold {fold_idx}] [{split}] No sequences usable for t={t}\")\n",
    "    dyn = _dynamic_eval(outdir, fold_idx, split, DECISION_THRESH, STABLE_K, DYN_MAX_FRAMES)\n",
    "    if dyn is not None:\n",
    "        _print_dynamic_block(f\"FOLD {fold_idx} | {split.upper()} | DYNAMIC (thr={DECISION_THRESH},K={STABLE_K},tmax={DYN_MAX_FRAMES})\",\n",
    "                             dyn, RANGE_BINS_METERS, CENTER_ANG_DEG, fps=fps_for_seconds)\n",
    "    else:\n",
    "        print(f\"[fold {fold_idx}] [{split}] No sequences produced a dynamic decision.\")\n",
    "\n",
    "# ----------------------------- Main -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_fine_grained_all_folds()\n",
    "    FPS_FOR_SECONDS = 10.0\n",
    "    for k in range(1, K_FOLDS+1):\n",
    "        run_fixed_horizon_and_dynamic_reports(k, split=\"val\", fps_for_seconds=FPS_FOR_SECONDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-gpu)",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
